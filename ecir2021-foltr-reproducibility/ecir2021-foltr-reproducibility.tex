\documentclass[runningheads]{llncs}
\usepackage{graphbox,graphicx}
\usepackage[labelfont=bf]{caption}
\usepackage{color}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[utf8]{inputenc}
%\usepackage{fourier} 
\usepackage{array}
\usepackage{makecell}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{scalerel}
\usepackage{float}
\usepackage{amsmath, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry} %amsthm
\usepackage{bm}
\usepackage{cite}
%\usepackage{subfigure}
\graphicspath{ {./images/} }
\usepackage{tabularx}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=40pt}
\usepackage{subcaption}

%\lstset{
%	basicstyle=\scriptsize\ttfamily,
%	columns=fullflexible,
%	frame=single,
%	moredelim=[is][\itshape]{[*}{*]},
%	breaklines=true,
%	breakindent=0pt
%}

\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red}{#1}}}

\newcommand{\customsymbol}[1]{\scalerel*{\includegraphics{#1}}{`}}
\begin{document}

%\title{An Experimental Study for Federated Online Learning to Rank with Evolution Strategies}

\title{Federated Online Learning to Rank with Evolution Strategies: A Reproducibility Study}


%
%\author{You You\inst{1}\orcidID{} \and
%		Guido Zuccon\inst{1}\orcidID{0000-0003-0271-5563} \and
%		XX\inst{1}\orcidID{} \and
%		XX\inst{1}\orcidID{}}

%\institute{The University of Queensland, St Lucia, Australia}


\maketitle

\begin{abstract}
Online Learning to Rank (OLTR) optimizes ranking models using users' online feedback directly, which has the advantages of high-efficiency and lower cost. The largely existing methods focus on improving the effectiveness and users' experience, more specifically the ranking quality in OLTR. Fewer attention has been paid to protecting users' privacy in the condition of guaranteeing the ranking qualify.

A recent work by Kharitonov~\cite{kharitonov2019federated} proposes a learning algorithm called FOLtR-ES that conducts Federated Machine Learning paradigm into OLTR in order to provide a solution in user privacy guarantees. This work develops a privatization procedure to guarantee $\epsilon$-local differential privacy.

Our experiments study the reproducibility of FOLtR-ES using MSLR-WEB10K and Yahoo! Webscope datasets. For a better study in performance of FOLtR-ES comparing to other widely-used OLTR methods, we train Pairwise Differentiable Gradient Descent (PDGD) models as baselines. We also extend the online optimization metric to Discounted Cumulative Gain and use nDCG as evaluation metric to study the consistency of ranking performance in FOLtR-ES. Our experiments show FOLtR-ES can generalise on larger datasets and other common online metric. Although FOLtR-ES preserves users' privacy to some degree, it lags behind the OLTR baseline in terms of ranking performance. 

	\keywords{Online Learning to Rank; Federated Machine Learning}
\end{abstract}


\input{sections/introduction.tex}
\input{sections/method.tex}

\input{sections/evaluation.tex}

\input{sections/results.tex}
\input{sections/related_works.tex}

\input{sections/conclusions.tex}

\makeatletter
\renewcommand{\@biblabel}[1]{\hfill #1.}
\makeatother

\subsubsection*{Acknowledgements.}


\bibliographystyle{splncs04}
\bibliography{ecir2021-foltr-reproducibility}


%\end{thebibliography}

\end{document}

