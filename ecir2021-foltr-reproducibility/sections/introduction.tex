\section{Introduction}

Online learning to rank (OLTR) exploits users queries and interactions with search engine result pages (SERPs) to iteratively train and update a ranker in  production~\cite{bibid}. In particular, OLTR relies on implicit user feedback from interactions on SERPs, e.g., clicks, rather than editorial relevance labels. 
Several methods for OLTR exist that attempt to address the specific challenges of online learning~\cite{bibid}: from making sense of the implicit feedback, to exploring the space of feature weights, accounting for biases in the click signal, reducing the impact of the online learning process on user experience, among others. 

An aspect that has not been received wide attention in OLTR is how the privacy of users could be guaranteed. Current OLTR methods in fact assume that a central server collects all queries and interactions of all users of the search system, and it is this central server that is responsible for the indexing of the collection, the training of the ranker and the production of the SERPs. A recent work by Kharitonov, however, has attempted to provide a mechanism for OLTR that preserves the privacy of users~\cite{kharitonov2019federated}. The method relies on the federated learning paradigm~\cite{bibid}, in which data (collection, queries, interactions) is maintained at each client's side along with a copy of the ranker, and updates to the rankers that are learned from the interaction on the client side are shared to the central server, which is responsible for aggregating the update signal from clients and propagate the aggregated ranker update. In this specific case, all users observe and act on the same feature space; each user however retains control of their own data, which includes the collection, the queries and the interactions.  





The following research questions are addressed in this paper:

RQ1. Does the Federated Online Learning to Rank algorithm also work on other widely used Learning to Rank datasets?
RQ2. Does the number of clients affect the performance of Federated Online Learning to Rank?
RQ3. Can federated online learning to rank achieve similar performance comparing with other state-of-the-art online learning to rank methods?
RQ4. Can Federated Online Learning to Rank generalise to other typical OLTR evaluation setup, such as nDCG or MRR?
