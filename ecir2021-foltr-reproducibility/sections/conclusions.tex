\section{Conclusions}

In this paper we considered the federated online learning to rank with evolutionary strategies (FOLTR-ES) method recently proposed by Kharitonov~\cite{kharitonov2019federated}. This is an interesting method because privacy requirements have been so far ignored in OLTR, and FOLTR-ES represents the first method of its kind. 

We set to explore four research questions related to FOLTR-ES. RQ1 aimed to investigate the generalisability of the original results obtained by FOLTR-ES on the MQ2007/2008 dataset to other datasets used in current OLTR practice. Our  experiments on MQ2007/2008 show consistent findings with that of Kharitonov~\cite{kharitonov2019federated}. However, when larger LTR datasets are considered, results change. In particular, the neural ranker used in FOLTR-ES is less effective than the linear ranker, especially on MSLR-WEB10K. %which shows FOLtR-ES needs more data to achieve effective training on those datasets with larger number of features.

RQ2 aimed to investigate the effect varying the number of clients involved in FOLTR-ES has on the effectiveness of the method. %We set the total times of interaction to a fix number and discover the ranking quality in terms of different number of clients in online simulation. 
Our experiments show mixed results with respect to the number of clients: the effect of the number of clients varies depending on dataset, ranker type and click settings. 
%that little clients number harm the performance in the linear ranker. But for the neural ranker, the difference is minor.

RQ3 aimed to compare FOLRT-ES with current OLTR state-of-the-art methods to understand the gap required to be paid for maintaining privacy. Our experiments show that FOLTR-ES lags behind the current OLTR state-of-the-art in terms of ranking performance: differences become more substantial when noisy clicks or larger datasets are considered. 

RQ4 aimed to investigate the generalisability of the original results obtained for FOLTR-ES to common evaluation practice in OLTR. Our experiments show that if the common evaluation settings used in OLTR are used to evaluate FOLTR-ES, then thee method shows high variability in effectiveness across datasets, rankers and clicks types -- and overall that FOLTR-ES is unreliable on large datasets and noisy clicks. This finding suggests that more research and improvements are needed before a federated OLTR method, and FOLTR-ES in particular, can be used in practice. 

Code, experiment scripts and results are provided at \url{https://github.com/ielab/foltr}. 

%Our experiments show FOLtR-ES can be easily applied to other common online metric and evaluated by offline performance metric. Still, FOLTR-ES performs behind than OLTR baseline in terms of nDCG metric.