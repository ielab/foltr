\section{Conclusions}

We set to explore 4 research questions. RQ1 aimed to investigate the generalisability of the original results obtained  by FOLTR-ES on the MQ2007/2008 dataset to other dataset used in current OLTR practice. Our reproducing experiments on MQ2007/2008 show the consistently similar findings with the original work. However, in terms of larger LTR datasets (MSLR-WEB10K and Yahoo datasets), the neural ranker is less effective than the linear ranker, especially on MSLR-WEB10K, which shows FOLtR-ES needs more data to achieve effective training on those datasets with larger number of features.
RQ2 aimed to investigate the effect varying the number of clients involved in FOLTR-ES has on the effectiveness of the method. We set the total times of interaction to a fix number and discover the ranking quality in terms of different number of clients in online simulation. Our experiments show that little clients number harm the performance in the linear ranker. But for the neural ranker, the difference is minor.
RQ3 aimed to compare FOLRT-ES with current OLTR state-of-the-art methods to understand the gap required to be paid for maintaining privacy. Our findings suggest that FOLtR-ES lags behind the OLTR baseline in terms of ranking performance
RQ4 aimed to investigate the generalisability of the original results obtained for FOLTR-ES to common evaluation practice in OLTR. \todo{Our findings suggest that ...}