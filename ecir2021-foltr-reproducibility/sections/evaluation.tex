\section{Experimental Settings}

In this section, we introduce the experimental settings to answer the research questions addressed in Section. 1.

\subsection{Datasets}
The original paper conducted experiments on MQ2007 and MQ2008 learning to rank dataset~\cite{DBLP:journals/corr/QinL13} which arguably small and outdated. In this paper, we instead use more recent and lager dataset: MSLR-WEB10K\cite{DBLP:journals/corr/QinL13} and Yahoo! Webscope~\cite{DBLP:journals/jmlr/ChapelleC11}, which are commonly-used in offline learning to rank and online learning to rank~\cite{zhuang2020counterfactual,DBLP:conf/wsdm/HofmannSWR13,jagerman2019model,oosterhuis2018differentiable}. Compare to MQ2007 and 2008, both MSLR-WEB10K and Yahoo! are using 5-level graded relevance judgements range from 0 (not relevant) to 4 (perfectly relevant).
Each dataset contains a centain much more number of queries and corresponding candidate documents: MSLR-WEB10K has 10,000 queries with each query having 125 assessed documents on average, and Yahoo has 29,921 queries with 709,877 documents. In addition, both datasets have much rich of features.  MSLR-WEB10K has 136 features and Yahoo has 700. We also repeat exemptions on MQ2007 for reproducing the experiments from the original work.
%More detailed information for each dataset can be found in Table \ref{table:1:dataset}.

%MQ2007 dataset contains 1692 unique queries and 69623 relevance labels. MQ2008 dataset contains 784 unique queries and 15211 relevance labels. For MQ2007 and MQ2008 dataset, each query-document pair contains 46 features.


%\begin{table}
%	\caption{Information for each dataset}
%	\label{table:1:dataset}
%	\centering
%	\begin{tabular}{l c c c}
%		%\lsptoprule
%		\hline
%		& & number of & \\
%		& queries&  labels  & features \\
%		\midrule
%		MQ2007  &   1,692  &    69,623  &    46    \\
%		%\hline
%		MSLR-WEB10K  &   10,000 &   9m  &    136  \\
%		%\hline
%		Yahoo  &   29,921 &   9m  &    700  \\
%		%\lspbottomrule
%		\hline
%	\end{tabular}
%\end{table}

\subsection{Simulation}

Because no public datasets with search log is available for online learning to rank, we adopt the similar setup with~\cite{DBLP:conf/wsdm/SchuthOWR16, DBLP:conf/wsdm/HofmannSWR13} - simulating users and their reaction to the search results using labelled offline learning to rank datasets.

We follow the same method with FOLtR-ES for simulation. We sample $B$ queries for each client randomly and use the local perturbed model to rank the documents. The length for each ranking list is limited to 10 documents. After simulating user's clicks data, we record the quality metric for each interaction and perform the privatization procedure with probability $p$. Next, we send the averaged metric and pseudo-random seed to optimize the centralized ranking model. Finally, each client receive the updated ranking model. 

For users' click simulation, we use Cascade Click Model (CCM)~\cite{DBLP:conf/wsdm/GuoLW09}. We run instances of CCM using the same click probability and stop probability for MSLR-WEB10K and Yahoo!. Under CCM, users are assumed to go through the ranking list from top to bottom. Each document is examined and clicked with click probability $P(click = 1 | r)$, conditioned on relevance label $r$. After a click, the user stops with stop probability $P(stop = 1 | r)$ or continues otherwise. Usually three instantiations of CCM are considered in OLTR: a $perfect$ user with very reliable feedback, a $navigational$ user searching for reasonably relevant documents, and an $informational$ user with noisiest feedback among three instantiations.  Table \ref{mslr-CCM} show the parameters of three click models. For clicks simulation with MQ2007, we use the same parameter settings from the table 1 in original paper.


\newcommand{\tc}[1]{\multicolumn{1}{c}{#1}}
\setlength{\tabcolsep}{3mm}

\begin{table}[t!]
	\centering
	\caption[centre]{Three click model instantiations for MSLR-WEB10K and Yahoo! dataset}\label{mslr-CCM}
	\begin{tabularx}{\textwidth}{XXXXXXXXXXX}
		\toprule
		& \multicolumn{5}{c}{$p(click=1|R)$} & \multicolumn{5}{c}{$p(stop=1|R)$} \\
		\cmidrule(r){2-6}  \cmidrule(){7-11}
		R & \tc{0}& \tc{1} &\tc{2} & \tc{3}& \tc{4}&  \tc{0} & \tc{1} & \tc{2} & \tc{3} & \tc{4} \\
		\midrule
		$perfect$ & 0.0 & 0.2 & 0.4 & 0.8 & 0.1& 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\
		$nav$ & 0.05 & 0.3 & 0.5 & 0.7 & 0.95& 0.2 & 0.3 & 0.5 & 0.7 & 0.9\\
		$infor$ & 0.4 & 0.6 & 0.7 & 0.8 & 0.9& 0.1 & 0.2 & 0.3 & 0.4 & 0.5\\
		\bottomrule
	\end{tabularx}
	\vspace{-10pt}
\end{table}

\subsection{Evaluation metric}
As FOLtR-ES is designed to allow optimization based on any absolute measures of ranking quality, we can extend the original methods to other widely used evaluation metrics for OLTR. For better reproducing and comparison with the original algorithm, we choose the reciprocal rank of the highest clicked result in each interaction, which is the same quality metric used in the original methods. We also use Discounted Cumulative Gain (DCG) to evaluate the users' experience in each interaction and NDCG@10 to evaluate the central ranker as we want to explore the performance of FOLtR-ES on other metrics.

\subsection{Baselines}

To further study how far the FOLtR-ES is left behind with state-of-the-art OLTR methods on ranking quality, we train Pairwise Differentiable Gradient Descent (PDGD)~\cite{oosterhuis2018differentiable} models as the baselines. Unlike many preview OLTR methods that are designed for linear models, PDGD also provides effective optimization for non-linear models such as neural models. During each interaction, a weighted differentiable pairwise loss is constructed in PDGD and the gradient is directly estimated by document pairs preferences inferred from user clicks. PDGD is empirically found to be significantly better than traditional OLTR methods in terms of final convergence, learning speed and user experience during optimization, making PDGD the current state-of-the-art method for OLTR~\cite{jagerman2019model,zhuang2020counterfactual}.
 
\subsection{Models and Optimization}
As we do not want to study the influence of ranking models to the original algorithm, we adopt the same models and optimization steps as~\cite{kharitonov2019federated}. The two ranking models are a linear ranker and a neural network with a single hidden layer of size 10. For optimization, we use Adam~\cite{kingma2014adam} with default parameters.

