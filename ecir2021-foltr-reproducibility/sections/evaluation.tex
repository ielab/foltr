\section{Experimental Settings}

%<<<<<<< HEAD
%In this section, we introduce the experimental settings to answer the research questions addressed in Section. 1.\footnote{The code for reproducing the experiments in this paper are available at~\url{https://github.com/Karlywang/foltr-reproduce}} 
%=======
%In this section, we introduce the experimental settings to answer the research questions addressed in Section. 1.\footnote{The code for reproducing the experiments in this paper are available at~\url{https://github.com/ielab/foltr}} 
%>>>>>>> 43ff9dbe5c5558c59adce8d48de65ad5d6611c33

\subsection{Datasets}
The original work of Kharitonov~\cite{kharitonov2019federated} conducted experiments on the MQ2007 and MQ2008 learning to rank datasets~\cite{DBLP:journals/corr/QinL13}, which are arguably small and outdated. In our work, we instead consider more recent and lager datasets: MSLR-WEB10K~\cite{DBLP:journals/corr/QinL13} and Yahoo! Webscope~\cite{DBLP:journals/jmlr/ChapelleC11}, which are commonly-used in offline and online learning to rank~\cite{zhuang2020counterfactual,DBLP:conf/wsdm/HofmannSWR13,jagerman2019model,oosterhuis2018differentiable}. Compared to MQ2007/2008, both MSLR-WEB10K and Yahoo! use 5-level graded relevance judgements, ranging from 0 (not relevant) to 4 (perfectly relevant).
Each dataset contains many more queries and corresponding candidate documents than MQ2007/2008: MSLR-WEB10K has 10,000 queries, with each query having 125 assessed documents on average, while Yahoo! has 29,921 queries with 709,877 documents. In addition, both datasets have much richer and numerous features. MSLR-WEB10K has 136 features and Yahoo! 700. For direct comparison with the original FOLTR-ES work, we also use MQ2007/2008.
%More detailed information for each dataset can be found in Table \ref{table:1:dataset}.

%MQ2007 dataset contains 1692 unique queries and 69623 relevance labels. MQ2008 dataset contains 784 unique queries and 15211 relevance labels. For MQ2007 and MQ2008 dataset, each query-document pair contains 46 features.


%\begin{table}
%	\caption{Information for each dataset}
%	\label{table:1:dataset}
%	\centering
%	\begin{tabular}{l c c c}
%		%\lsptoprule
%		\hline
%		& & number of & \\
%		& queries&  labels  & features \\
%		\midrule
%		MQ2007  &   1,692  &    69,623  &    46    \\
%		%\hline
%		MSLR-WEB10K  &   10,000 &   9m  &    136  \\
%		%\hline
%		Yahoo  &   29,921 &   9m  &    700  \\
%		%\lspbottomrule
%		\hline
%	\end{tabular}
%\end{table}

\subsection{Simulation}

Because no public datasets with search log is available for online learning to rank, we adopt the similar setup with~\cite{DBLP:conf/wsdm/SchuthOWR16, DBLP:conf/wsdm/HofmannSWR13} - simulating users and their reaction to the search results using labelled offline learning to rank datasets.

We follow the same method with FOLtR-ES for simulation. We sample $B$ queries for each client randomly and use the local perturbed model to rank the documents. The length for each ranking list is limited to 10 documents. After simulating user's clicks data, we record the quality metric for each interaction and perform the privatization procedure with probability $p$. Next, we send the averaged metric and pseudo-random seed to optimize the centralized ranking model. Finally, each client receive the updated ranking model. 

For users' click simulation, we use Cascade Click Model (CCM)~\cite{DBLP:conf/wsdm/GuoLW09}. We run instances of CCM using the same click probability and stop probability for MSLR-WEB10K and Yahoo!. Under CCM, users are assumed to go through the ranking list from top to bottom. Each document is examined and clicked with click probability $P(click = 1 | r)$, conditioned on relevance label $r$. After a click, the user stops with stop probability $P(stop = 1 | r)$ or continues otherwise. Usually three instantiations of CCM are considered in OLTR: a $perfect$ user with very reliable feedback, a $navigational$ user searching for reasonably relevant documents, and an $informational$ user with noisiest feedback among three instantiations.  Table \ref{mslr-CCM} show the parameters of three click models, $Perfect$, $Navigational$, and $Informational$. For clicks simulation with MQ2007, we use the same parameter settings from the Table 1 in the original paper.


\newcommand{\tc}[1]{\multicolumn{1}{c}{#1}}
\setlength{\tabcolsep}{3mm}

\begin{table}[t!]
	\centering
	\caption[centre]{Three click model instantiations for MSLR-WEB10K and Yahoo! dataset}\label{mslr-CCM}
	\begin{tabularx}{\textwidth}{XXXXXXXXXXX}
		\toprule
		& \multicolumn{5}{c}{$p(click=1|R)$} & \multicolumn{5}{c}{$p(stop=1|R)$} \\
		\cmidrule(r){2-6}  \cmidrule(){7-11}
		R & \tc{0}& \tc{1} &\tc{2} & \tc{3}& \tc{4}&  \tc{0} & \tc{1} & \tc{2} & \tc{3} & \tc{4} \\
		\midrule
		$perf$ & 0.0 & 0.2 & 0.4 & 0.8 & 0.1& 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\
		$nav$ & 0.05 & 0.3 & 0.5 & 0.7 & 0.95& 0.2 & 0.3 & 0.5 & 0.7 & 0.9\\
		$inf$ & 0.4 & 0.6 & 0.7 & 0.8 & 0.9& 0.1 & 0.2 & 0.3 & 0.4 & 0.5\\
		\bottomrule
	\end{tabularx}
	\vspace{-10pt}
\end{table}

\subsection{Evaluation metric}

For better reproducing and comparison with the original algorithm, we choose the reciprocal rank of the highest clicked result in each interaction, which is the same quality metric used in the original methods. 

To further explore the effectiveness of FOLtR-ES on other widely used evaluation metrics for OLTR. We also train the model to optimize Discounted Cumulative Gain (DCG) of users experience during training, as FOLtR-ES is designed to allow optimization based on any absolute measures of ranking quality. For this part of experimental runs, on the one hand, we record nDCG@10 scores of the displaced rankings during training as the scores represent users' satisfaction, this is refer to as online nDCG score~\cite{DBLP:conf/wsdm/HofmannSWR13}, on the other hand, we also record the nDCG@10 of the final learned ranker on a heldout test set, this is refer to as offline nDCG score.
\subsection{Baselines}

To further study how far the FOLtR-ES is left behind with state-of-the-art OLTR methods on ranking quality, we train Pairwise Differentiable Gradient Descent (PDGD)~\cite{oosterhuis2018differentiable} models as the baselines. Unlike many preview OLTR methods that are designed for linear models, PDGD also provides effective optimization for non-linear models such as neural models. During each interaction, a weighted differentiable pairwise loss is constructed in PDGD and the gradient is directly estimated by document pairs preferences inferred from user clicks. PDGD is empirically found to be significantly better than traditional OLTR methods in terms of final convergence, learning speed and user experience during optimization, making PDGD the current state-of-the-art method for OLTR~\cite{jagerman2019model,zhuang2020counterfactual}.
 
\subsection{Models and Optimization}
As we do not want to study the influence of ranking models to the original algorithm, we adopt the same models and optimization steps as~\cite{kharitonov2019federated}. The two ranking models are a linear ranker and a neural network with a single hidden layer of size 10. For optimization, we use Adam~\cite{kingma2014adam} with default parameters.

