\section{Experimental Settings}

In this section, we introduce the experimental settings to answer the research questions addressed in Section. 1.

\subsection{Datasets}
The datasets we use are MQ2007 and MQ2008~\cite{DBLP:journals/corr/QinL13}, MSLR-WEB10K\cite{DBLP:journals/corr/QinL13} and Yahoo! Webscope~\cite{DBLP:journals/jmlr/ChapelleC11} datasets, which are commonly-used offline learning to rank datasets. Each dataset contains a centain amount of queries and corresponding candidate documents. The query-document pairs are represented by feature vectors and relevance labels. For MQ2007 and MQ2008 datasets, the relevance labels value as 0 (not relevant), 1 (partially relevant) and 2 (perfecly relevant). The relevance annotations of MSLR-WEB10K range from 0 (not relevant) to 4 (perfectly relevant). Each dataset contains five splits, except for Yahoo! Webscope. More detailed information for each dataset can be found in Table \ref{table:1:dataset}.

%MQ2007 dataset contains 1692 unique queries and 69623 relevance labels. MQ2008 dataset contains 784 unique queries and 15211 relevance labels. For MQ2007 and MQ2008 dataset, each query-document pair contains 46 features.
We use the original dataset in training and validating procedure without any preprocessing, such as feature selection or feature normalization.

\begin{table}
	\caption{Information for each dataset}
	\label{table:1:dataset}
	\centering
	\begin{tabular}{l c c c}
		%\lsptoprule
		\hline
		& & number of & \\
		& queries&  relevance labels  & features \\
		\midrule
		MQ2007  &   1692  &    69623  &    46    \\
		%\hline
		MQ2008  &  784  &   15211  &    46    \\
		%\hline
		MSLR-WEB10K  &   XXX &   XXX  &    136  \\
		%\lspbottomrule
		\hline
	\end{tabular}
\end{table}

\subsection{Simulation}

Because no public datasets with search log is available for online learning to rank, we adopt the similar setup with~\cite{DBLP:conf/wsdm/SchuthOWR16, DBLP:conf/wsdm/HofmannSWR13} - simulating users and their reaction to the search results using labelled offline learning to rank datasets.

We follow the same method with FOLtR-ES for simulation. We sample $B$ queries for each client randomly and use the local perturbed model to rank the documents. The length for each ranking list is limited to 10 documents. After simulating user's clicks data, we record the quality metric for each interaction and perform the privatization procedure with probability $p$. Next, we send the averaged metric and pseudo-random seed to optimize the centralized ranking model. Finally, each client receive the updated ranking model. 

For users' click simulation, we use Cascade Click Model (CCM)~\cite{DBLP:conf/wsdm/GuoLW09}. We run instances of CCM using the same click probability and stop probability with~\cite{kharitonov2019federated} for MQ2007 and~\cite{oosterhuis2016probabilistic} for MSLR-WEB10K. Under CCM, users are assumed to go through the ranking list from top to bottom. Each document is examined and clicked with click probability $P(click = 1 | r)$, conditioned on relevance label $r$. After a click, the user stops with stop probability $P(stop = 1 | r)$ or continues otherwise. Usually three instantiations of CCM are considered in OLTR: a $perfect$ user with very reliable feedback, a $navigational$ user searching for reasonably relevant documents, and an $informational$ user with noisiest feedback among three instantiations.  table 2 shows the parameters of three click models.

\begin{table}
	\caption{Three click model instantiations for MQ2007 dataset}
	\label{table:2:mq2007}
	\centering
	\begin{tabular}{l c c c c c c}
		%\lsptoprule
		\hline
		&&$P(click = 1 | r)$ &&& $P(stop = 1 | r)$  & \\
		& r = 0&  r = 1  & r = 2 & r = 0&  r = 1  & r = 2\\
		\midrule
		perfect  &   0.0  &    0.5  &    1.0 & 0.0 & 0.0 & 0.0   \\
		%\hline
		navigational  &  0.05  &   0.5  &    0.95  & 0.2 & 0.5 & 0.9  \\
		%\hline
		informational  &   0.4 &   0.7  &    0.9 & 0.1 & 0.3 & 0.5 \\
		%\lspbottomrule
		\hline
	\end{tabular}
\end{table}

\begin{table}
	\caption{Three click model instantiations for MSLR-WEB10K dataset}
	\label{table:3:MSLR-WEB10K}
	\centering
	\begin{tabular}{l c c c c c c c c c c}
		%\lsptoprule
		\hline
		&&&$P(click = 1 | r)$ &&&&& $P(stop = 1 | r)$  && \\
		& r = 0&  r = 1  & r = 2 & r = 3 & r = 4 & r = 0&  r = 1  & r = 2 & r = 3 & r = 4\\
		\midrule
		perfect  &   0.0  &    0.2  &   0.4  & 0.8 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
		%\hline
		navigational  &  0.05  &   0.3  &    0.5  & 0.7 & 0.95 & 0.2  & 0.3  & 0.5  & 0.7 & 0.9  \\
		%\hline
		informational  &   0.4 &   0.6  &   0.7 & 0.8 & 0.9  & 0.1  & 0.2 & 0.3 & 0.4 & 0.5 \\
		%\lspbottomrule
		\hline
	\end{tabular}
\end{table}

\subsection{Evaluation metric}
As FOLtR-ES is designed to allow optimization based on any absolute measures of ranking quality, we can extend the original methods to other widely used evaluation metrics for OLTR. For better reproducing and comparison with the original algorithm, we choose the reciprocal rank of the highest clicked result in each interaction, which is the same quality metric used in the original methods. We also use Discounted Cumulative Gain (DCG) to evaluate the users' experience in each interaction and NDCG@10 to evaluate the central ranker as we want to explore the performance of FOLtR-ES on other metrics.

\subsection{Baselines}
To further study how far the FOLtR-ES is left behind with common OLTR methods on ranking quality, we train Pairwise Differentiable Gradient Descent (PDGD)~\cite{oosterhuis2018differentiable} models as the baselines.

\subsection{Models and Optimization}
As we do not want to study the influence of ranking models to the original algorithm, we adopt the same models and optimization steps as~\cite{kharitonov2019federated}. The two ranking models are a linear ranker and a neural network with a single hidden layer of size 10. For optimization, we use Adam~\cite{kingma2014adam} with default parameters.


